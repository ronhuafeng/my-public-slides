## Model Context Protocol 的基本知识



## 业务实践

### 使用 MCP
业务实践中除了让大模型生成代码，可能还需要大模型辅助进行错误调试、业务流程优化、运维等等。这些任务需要感知业务的运行状态，而将这些运行状态输入给大模型可能需要 MCP（或者手工复制粘贴给大模型也可以）。

我们可以通过下面的流程来实现这些目标：

- 使用第三方的 MCP 完成服务，这些第三方的 MCP 实现可能已经设计比较完善了
- 通过提示词注入，然后利用一些内置的通用 tool 去处理这些注入的提示。例如写入一些列服务启动、调试命令脚本在提示词，大模型可以通过 execute_command_line 工具或者 python_exec 依据提示去生成脚本来运行调试过程。

### 自定义 MCP

为什么要业务实践中自定义 MCP 服务？

- 首先这不是一个必须进行的操作，上面的方案在一定程度上能够完成任务

- 我们的目的是让整个过程更加可控，可以将多个步骤封装在 tool，减少大模型规划的压力
  - 例如将系统启动（docker compose）、用真实数据去请求系统响应、查看系统的日志（可能包含错误日志）这几个步骤封装成一个 tool。大模型在修改代码后，调用工具查看修改效果，然后可以根据系统的日志进行反馈，进一步对自己的修改进行迭代。
    - 这个可以用在代码优化、重构、缺陷调试中，可以有效构建一个大模型和真实系统间的反馈循环

### 一个案例：构建反馈循环，利用大模型自动进行算法优化

一个案例：

模块任务：给定一组验证规则，使用一个 agentic workflow 去验证每个 Excel 文件是否满足这些规则，并给判断的依据。

规则举例：

- 标注为“数量”的列不为空
- 表格中 “完成”需要 打勾 ☑

这个任务涉及到 Excel 文件的读取和分析。可以使用正则表达式提取、大模型判断或者一些复合手段来给出最后的判断。

- 正则匹配 完成，然后在前后匹配 ☑ 或者类似的字符
- 将 Excel 转成文本，加上提示词，让大模型去判断

现实的问题：

- Excel 文件排版不一致，导致定位特定的列的算法并不稳定
- 文件中一个 sheet 可能有多个“区域”，即一个 sheet 其实放下了多个 sheet 的内容，区域划分是通过带有颜色的单元格合并后模拟的边界。因此判断一列的结束需要一些技巧。

人去一个个分析这些文件的异同比较繁琐。一个可行的流程是：

- 先构建一个测试脚本，输入为检查算法原型（可以没有任何实际的实现）、每个检查算法对应的规则、一组测试用的 Excel 文件、预期的判断结果
- Copilot
  - 执行 <检查算法, 算法规则描述, 文件> -> 获得输出结果（一开始的时候可能都是错的）
  - 根据 <算法描述, 预期结果> 重新修改算法实现，应用在文件上 -> 获得修改后的输出结果
  - 不断重复上面的循环，知道算法实现能够正确判别文件

遇到过的问题：

- ❌ 强行拟合：算法根据文件名去给出特定规则的判别结果
- ❌ 使用了大量关键词进行匹配，实现算法不通用
- ✅ 生成了一些非常有效的 Excel 文件处理片段：提取表格 sheet 名称、理解合并单元格、理解 Cell 颜色等等，这些都能做成一组 Excel  理解的工具

这个过程中，也可以将自己对 Excel 文件的一些理解记录下来，作为上下文，交给 Copilot，让它产生一些灵感。

参考：Google Deepmind：《AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms》


## 个人体验：好用的流程来自失败的尝试

1. 大模型有时会过渡设计：可以在提示词里要求 KISS Principle
2. 上下文受限时，无法兼顾所有需要调整的代码调用地点
  - 回归测试，然后进行继续迭代
  - 送到 Gemini 2.5 Pro 中，利用 100M 上下文去处理
3. 要人工 review 生成的代码

## 人和  AI 的互相教育

Prompt -> AI

- AI 学到了什么，我其实不太清楚

Result -> Human

我的技术提升：

- alembic
- python 中的 pydantic 类型系统支持
- classmethod
- 异步调用返回机制
- 理解 AI 的行为风格和能力边界

## 在探索过程中开发的辅助项目：

- command line tools
- proxy-in-middle
- mcp-sse-proxy
- vscode-copy-for-llm
- drawio-for-aistudio




